{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'spam.csv'\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['tokens'] = df['text'].apply(word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour lemmatiser une phrase\n",
    "def lemmatize_sentence(sentence):\n",
    "    return [lemmatizer.lemmatize(word) for word in sentence]\n",
    "\n",
    "# Appliquer la lemmatisation\n",
    "df['lemmas'] = df['tokens'].apply(lemmatize_sentence)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Fonction pour stemmatiser une phrase\n",
    "def stem_sentence(sentence):\n",
    "    return [stemmer.stem(word) for word in sentence]\n",
    "\n",
    "# Appliquer la stemmatisation\n",
    "df['stems'] = df['tokens'].apply(stem_sentence)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.title('Distribution des labels (Spam vs Ham)')\n",
    "plt.show()\n",
    "\n",
    "# Longueur des messages\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "sns.histplot(df[df['label'] == 'ham']['text_length'], kde=True, color='blue', label='Ham')\n",
    "sns.histplot(df[df['label'] == 'spam']['text_length'], kde=True, color='red', label='Spam')\n",
    "plt.legend()\n",
    "plt.title('Distribution de la longueur des messages')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['cleaned_tokens'] = df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Fréquence des mots\n",
    "all_words = [word for tokens in df['cleaned_tokens'] for word in tokens]\n",
    "word_freq = Counter(all_words)\n",
    "common_words = word_freq.most_common(20)\n",
    "\n",
    "# Afficher les mots les plus fréquents\n",
    "words, counts = zip(*common_words)\n",
    "plt.bar(words, counts)\n",
    "plt.title('20 mots les plus fréquents')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Fréquence des bigrams\n",
    "def get_ngrams(tokens, n=2):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "all_bigrams = [bigram for tokens in df['cleaned_tokens'] for bigram in get_ngrams(tokens, 2)]\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "common_bigrams = bigram_freq.most_common(20)\n",
    "\n",
    "# Afficher les bigrams les plus fréquents\n",
    "bigrams, counts = zip(*common_bigrams)\n",
    "plt.bar(bigrams, counts)\n",
    "plt.title('20 bigrams les plus fréquents')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x: x.lower())\n",
    "ham_words = Counter()\n",
    "ham_messages = df[df['label'] == 'ham']['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "for msg in ham_messages:\n",
    "    ham_words.update(msg)\n",
    "\n",
    "print(\"Mots les plus fréquents dans les messages 'ham':\")\n",
    "print(ham_words.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = Counter()\n",
    "spam_messages = df[df['label'] == 'spam']['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "for msg in spam_messages:\n",
    "    spam_words.update(msg)\n",
    "\n",
    "print(\"\\nMots les plus fréquents dans les messages 'spam':\")\n",
    "print(spam_words.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Télécharger le lexique VADER\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Fonction pour obtenir le score de sentiment\n",
    "def get_sentiment_scores(text):\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "df['sentiment'] = df['text'].apply(get_sentiment_scores)\n",
    "\n",
    "# Extraire les scores de sentiment\n",
    "df['neg'] = df['sentiment'].apply(lambda x: x['neg'])\n",
    "df['neu'] = df['sentiment'].apply(lambda x: x['neu'])\n",
    "df['pos'] = df['sentiment'].apply(lambda x: x['pos'])\n",
    "df['compound'] = df['sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[df['label'] == 'ham']['compound'], kde=True, color='blue', label='Ham')\n",
    "sns.histplot(df[df['label'] == 'spam']['compound'], kde=True, color='red', label='Spam')\n",
    "plt.legend()\n",
    "plt.title('Distribution des scores de sentiment (Compound)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages avec les scores de sentiment les plus positifs\n",
    "print(\"Messages les plus positifs:\")\n",
    "print(df.nlargest(5, 'compound')[['text', 'compound']])\n",
    "\n",
    "# Messages avec les scores de sentiment les plus négatifs\n",
    "print(\"Messages les plus négatifs:\")\n",
    "print(df.nsmallest(5, 'compound')[['text', 'compound']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from wordcloud import  ImageColorGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Convertir les listes de tokens en texte\n",
    "ham_text = ' '.join([' '.join(msg) for msg in ham_messages])\n",
    "spam_text = ' '.join([' '.join(msg) for msg in spam_messages])\n",
    "\n",
    "# Fonction pour afficher le nuage de mots\n",
    "def plot_word_cloud(text, title, mask_path):\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "    wc = WordCloud(width=600, height=600, background_color='white', max_words=200, stopwords=stop_words, mask=mask, max_font_size=90, collocations=False, random_state=42)\n",
    "    wc.generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=10, pad=10)\n",
    "    plt.show()\n",
    "\n",
    "# Afficher les nuages de mots\n",
    "plot_word_cloud(ham_text, 'Nuage de mots pour les messages ham', 'coeur.png')\n",
    "plot_word_cloud(spam_text, 'Nuage de mots pour les messages spam', 'coeur.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
