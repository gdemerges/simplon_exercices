{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import and Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/aug_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Dictionaries of ordinal features\n",
    "\n",
    "relevent_experience_map = {\n",
    "    'Has relevent experience':  1,\n",
    "    'No relevent experience':    0\n",
    "}\n",
    "\n",
    "experience_map = {\n",
    "    '<1'      :    0,\n",
    "    '1'       :    1,\n",
    "    '2'       :    2,\n",
    "    '3'       :    3,\n",
    "    '4'       :    4,\n",
    "    '5'       :    5,\n",
    "    '6'       :    6,\n",
    "    '7'       :    7,\n",
    "    '8'       :    8,\n",
    "    '9'       :    9,\n",
    "    '10'      :    10,\n",
    "    '11'      :    11,\n",
    "    '12'      :    12,\n",
    "    '13'      :    13,\n",
    "    '14'      :    14,\n",
    "    '15'      :    15,\n",
    "    '16'      :    16,\n",
    "    '17'      :    17,\n",
    "    '18'      :    18,\n",
    "    '19'      :    19,\n",
    "    '20'      :    20,\n",
    "    '>20'     :    21\n",
    "}\n",
    "\n",
    "last_new_job_map = {\n",
    "    'never'        :    0,\n",
    "    '1'            :    1,\n",
    "    '2'            :    2,\n",
    "    '3'            :    3,\n",
    "    '4'            :    4,\n",
    "    '>4'           :    5\n",
    "}\n",
    "\n",
    "# Transform categorical features into numerical features\n",
    "\n",
    "def encode(df_pre):\n",
    "    df_pre.loc[:,'relevent_experience'] = df_pre['relevent_experience'].map(relevent_experience_map)\n",
    "    df_pre.loc[:,'last_new_job'] = df_pre['last_new_job'].map(last_new_job_map)\n",
    "    df_pre.loc[:,'experience'] = df_pre['experience'].map(experience_map)\n",
    "\n",
    "    return df_pre\n",
    "\n",
    "df = encode(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define Sets of Columns to be Transformed in Different Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['city_development_index','relevent_experience', 'experience','last_new_job', 'training_hours']\n",
    "\n",
    "cat_cols = ['gender', 'enrolled_university', 'education_level', 'major_discipline', 'company_size', 'company_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create Pipelines for Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale',MinMaxScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one-hot',OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create ColumnTransformer to Apply the Pipeline for Each Column Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col_trans = ColumnTransformer(transformers=[\n",
    "    ('num_pipeline',num_pipeline,num_cols),\n",
    "    ('cat_pipeline',cat_pipeline,cat_cols)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Add a Model to the Final Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('col_trans', col_trans),\n",
    "    ('model', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Display the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "display(clf_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Split the Data into Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[num_cols+cat_cols]\n",
    "y = df['target']\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Pass Data through the Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline.fit(X_train, y_train)\n",
    "# preds = clf_pipeline.predict(X_test)\n",
    "score = clf_pipeline.score(X_test, y_test)\n",
    "print(f\"Model score: {score}\") # model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Save the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save pipeline to file \"pipe.joblib\"\n",
    "joblib.dump(clf_pipeline,\"pipe.joblib\")\n",
    "\n",
    "# Load pipeline when you want to use\n",
    "same_pipe = joblib.load(\"pipe.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "grid_params = {'model__penalty' : ['none', 'l2'],\n",
    "               'model__C' : np.logspace(-4, 4, 20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(clf_pipeline, grid_params, cv=5, scoring='accuracy')\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score of train set: \"+str(gs.best_score_))\n",
    "print(\"Best parameter set: \"+str(gs.best_params_))\n",
    "print(\"Test Score: \"+str(gs.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline2 = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('minmax_scale', MinMaxScaler()),\n",
    "    ('std_scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "col_trans2 = ColumnTransformer(transformers=[\n",
    "    ('num_pipeline',num_pipeline2,num_cols),\n",
    "    ('cat_pipeline',cat_pipeline,cat_cols)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1)\n",
    "\n",
    "clf_pipeline2 = Pipeline(steps=[\n",
    "    ('col_trans', col_trans2),\n",
    "    ('model', clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_step_params = [{'col_trans__num_pipeline__minmax_scale': ['passthrough']},\n",
    "                    {'col_trans__num_pipeline__std_scale': ['passthrough']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = GridSearchCV(clf_pipeline2, grid_step_params, scoring='accuracy')\n",
    "gs2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score of train set: \"+str(gs2.best_score_))\n",
    "print(\"Best parameter set: \"+str(gs2.best_params_))\n",
    "print(\"Test Score: \"+str(gs2.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {'model__penalty' : ['none', 'l2'],\n",
    "               'model__C' : np.logspace(-4, 4, 20)}\n",
    "\n",
    "grid_step_params = [{**{'col_trans__num_pipeline__minmax_scale': ['passthrough']}, **grid_params},\n",
    "                    {**{'col_trans__num_pipeline__std_scale': ['passthrough']}, **grid_params}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
